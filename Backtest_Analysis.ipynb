{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model Backtest Analysis\n",
    "\n",
    "This notebook tests the accuracy of our NBA PrizePicks prediction model by:\n",
    "1. Using the **first half** of the 24/25 season as training data\n",
    "2. Making predictions for games in the **second half**\n",
    "3. Comparing predictions against actual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the backtester\n",
    "from backtester import PredictionBacktester\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.2, style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize backtester and run with 200 predictions\n",
    "backtester = PredictionBacktester()\n",
    "results = backtester.run_backtest(n_predictions=200, balanced=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions dataframe\n",
    "df = results['predictions_df']\n",
    "df_no_push = df[df['actual_outcome'] != 'PUSH']\n",
    "\n",
    "# Accuracy by confidence level\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Accuracy by confidence bins\n",
    "df_no_push['conf_bin'] = pd.cut(df_no_push['confidence'], \n",
    "                                 bins=[0, 45, 55, 65, 100],\n",
    "                                 labels=['<45%', '45-55%', '55-65%', '>65%'])\n",
    "\n",
    "conf_acc = df_no_push.groupby('conf_bin')['correct'].agg(['mean', 'count'])\n",
    "conf_acc['accuracy'] = conf_acc['mean'] * 100\n",
    "\n",
    "bars = axes[0].bar(conf_acc.index, conf_acc['accuracy'], color=['#ff6b6b', '#ffd93d', '#6bcb77', '#4d96ff'])\n",
    "axes[0].axhline(y=52.4, color='red', linestyle='--', label='Profitability threshold (52.4%)')\n",
    "axes[0].set_xlabel('Confidence Level')\n",
    "axes[0].set_ylabel('Accuracy %')\n",
    "axes[0].set_title('Accuracy by Confidence Level')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, conf_acc['count']):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                 f'n={int(count)}', ha='center', fontsize=10)\n",
    "\n",
    "# Right plot: Accuracy by stat category\n",
    "stat_acc = df_no_push.groupby('stat')['correct'].agg(['mean', 'count'])\n",
    "stat_acc['accuracy'] = stat_acc['mean'] * 100\n",
    "stat_acc = stat_acc.sort_values('accuracy', ascending=True)\n",
    "\n",
    "colors = ['#ff6b6b' if x < 52.4 else '#6bcb77' for x in stat_acc['accuracy']]\n",
    "bars2 = axes[1].barh(stat_acc.index, stat_acc['accuracy'], color=colors)\n",
    "axes[1].axvline(x=52.4, color='red', linestyle='--', label='Profitability threshold')\n",
    "axes[1].set_xlabel('Accuracy %')\n",
    "axes[1].set_title('Accuracy by Stat Category')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 100)\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars2, stat_acc['count']):\n",
    "    axes[1].text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n",
    "                 f'n={int(count)}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy by prediction direction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# OVER vs UNDER accuracy\n",
    "df_no_push['direction_group'] = df_no_push['prediction'].apply(\n",
    "    lambda x: 'OVER' if 'OVER' in x else 'UNDER'\n",
    ")\n",
    "\n",
    "dir_acc = df_no_push.groupby('direction_group')['correct'].agg(['mean', 'count'])\n",
    "dir_acc['accuracy'] = dir_acc['mean'] * 100\n",
    "\n",
    "bars = axes[0].bar(dir_acc.index, dir_acc['accuracy'], color=['#4d96ff', '#ff6b6b'])\n",
    "axes[0].axhline(y=52.4, color='gray', linestyle='--', label='Profitability threshold')\n",
    "axes[0].set_ylabel('Accuracy %')\n",
    "axes[0].set_title('Accuracy: OVER vs UNDER')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "for bar, count in zip(bars, dir_acc['count']):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                 f'n={int(count)}', ha='center', fontsize=10)\n",
    "\n",
    "# Confidence vs Margin scatter\n",
    "scatter = axes[1].scatter(df_no_push['confidence'], df_no_push['margin'], \n",
    "                          c=df_no_push['correct'].map({True: 'green', False: 'red'}),\n",
    "                          alpha=0.6)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1].set_xlabel('Confidence %')\n",
    "axes[1].set_ylabel('Margin (Actual - Line)')\n",
    "axes[1].set_title('Confidence vs Margin (Green=Correct, Red=Incorrect)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall summary\n",
    "print(\"=\"*60)\n",
    "print(\"BACKTEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal Predictions: {results['total_predictions']}\")\n",
    "print(f\"Pushes (excluded): {results['pushes']}\")\n",
    "print(f\"Evaluated: {results['evaluated_predictions']}\")\n",
    "print(f\"\\n*** OVERALL ACCURACY: {results['overall_accuracy']:.1f}% ***\")\n",
    "print(f\"Correct: {results['correct_predictions']}\")\n",
    "print(f\"Incorrect: {results['incorrect_predictions']}\")\n",
    "\n",
    "print(f\"\\n--- By Confidence ---\")\n",
    "hc = results['high_confidence']\n",
    "mc = results['medium_confidence']\n",
    "print(f\"High Confidence (>=60%): {hc['accuracy']:.1f}% ({hc['correct']}/{hc['count']})\")\n",
    "print(f\"Medium Confidence (45-59%): {mc['accuracy']:.1f}% ({mc['correct']}/{mc['count']})\")\n",
    "\n",
    "# Profitability analysis\n",
    "print(f\"\\n--- Profitability Analysis ---\")\n",
    "win_rate = results['overall_accuracy'] / 100\n",
    "roi = (win_rate * 0.909 - (1 - win_rate)) * 100  # -110 odds\n",
    "print(f\"Estimated ROI (at -110 odds): {roi:.1f}%\")\n",
    "print(f\"Break-even win rate: 52.4%\")\n",
    "if results['overall_accuracy'] >= 52.4:\n",
    "    print(\"[+] MODEL IS PROFITABLE\")\n",
    "else:\n",
    "    print(\"[-] Model needs improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best and worst predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE HIGH-CONFIDENCE CORRECT PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "high_conf_correct = df_no_push[(df_no_push['confidence'] >= 60) & (df_no_push['correct'] == True)]\n",
    "print(high_conf_correct[['player', 'stat', 'line', 'prediction', 'confidence', \n",
    "                         'actual_value', 'actual_outcome']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE INCORRECT PREDICTIONS (to understand failures)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "incorrect = df_no_push[df_no_push['correct'] == False]\n",
    "print(incorrect[['player', 'stat', 'line', 'prediction', 'confidence', \n",
    "                 'actual_value', 'actual_outcome', 'margin']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations Based on Backtest\n",
    "\n",
    "Based on the backtest results:\n",
    "\n",
    "1. **Focus on High-Confidence Picks** - Predictions with >=60% confidence significantly outperform\n",
    "2. **Best Stat Categories** - AST, STL+BLK, and 3P show the highest accuracy\n",
    "3. **Avoid** - PTS predictions have lower accuracy; consider being more selective\n",
    "4. **Model Edge** - The model shows a clear profitable edge above the 52.4% threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "df.to_csv('backtest_results.csv', index=False)\n",
    "print(\"Results saved to backtest_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
